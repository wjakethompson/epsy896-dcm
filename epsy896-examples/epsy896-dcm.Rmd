---
title: "EPSY 896: Diagnostic Psychometrics"
output: html_document
date: "2024-03-26"
---

```{r setup, include=FALSE}
# install.packages(c("tidyverse", "measr", "here"))
library(tidyverse)
library(taylor)
library(measr)
library(here)

knitr::opts_chunk$set(echo = TRUE)
```

## Example data

The example data is a simulated assessment that measures 4 attributes (songwriting, production, vocals, cohesion) with 28 items.
The data contains responses to the 28 items for 500 albums.

```{r}
taylor_data <- read_rds(here("data", "taylor-data.rds"))
taylor_qmatrix <- read_rds(here("data", "taylor-qmatrix.rds"))

taylor_data

taylor_qmatrix
```

## Estimate the model

We can estimate models using `measr_dcm()`:

1. Specify your data, Q-matrix, and ID columns
2. Choose the DCM to estimate (e.g., LCDM, DINA, etc.)
3. Choose the estimation engine
4. Pass additional arguments to rstan or cmdstanr
5. Save the model to save time in the future

```{r}
ts_dcm <- measr_dcm(
  data = ecpe_data, qmatrix = ecpe_qmatrix, # <1>
  resp_id = "album",                        # <1>
  type = "lcdm",                            # <2>
  method = "mcmc", backend = "rstan",       # <3>
  iter = 2000, warmup = 1500,               # <4>
  chains = 4, cores = 4,                    # <4>
  file = "fits/taylor-lcdm"                 # <5>
)
```

## View predictions

```{r}
ts_results <- predict(ts_dcm, summary = FALSE)

ts_results$class_probabilities

ts_results$attribute_probabilities
```

## Absolute model fit

For the M2 statistic, we're looking for *p*-values greater than .05.

```{r}
fit_m2(ts_dcm)
```

For PPMCs, we're looking for *ppp* values between .025 and .975.

```{r}
fit_ppmc(ts_dcm, model_fit = "raw_score", item_fit = NULL)
```

## Relative fit

First, let's fit a competing model.

```{r}
ts_dina <- measr_dcm(
  data = ts_dat, qmatrix = ts_qmat,
  resp_id = "album",
  type = "dina",
  method = "mcmc", backend = "cmdstanr",
  iter_warmup = 1500, iter_sampling = 500,
  chains = 4, parallel_chains = 4,
  file = "fits/taylor-dina"
)
```

Compare the two models using the LOO criterion.

```{r}
lcdm_loo <- loo(ts_dcm)
dina_loo <- loo(ts_dina)

loo_compare(list(lcdm = lcdm_loo, dina = dina_loo))
```

The LCDM is the preferred model, because it is listed first in the output.
Because the absolute value of the difference is greater than 2.5 times the standard error of the difference, we conclude that the LCDM is significantly better.

## Reliability

The type of reliability needed will depend on how scores are reported.

```{r}
ts_reli <- reliability(ts_dcm)
```

If scores are calculated using the most likely profile, than we should report profile-level reliability.

```{r}
ts_reli$pattern_reliability
```

If scores are calculated for each attribute individually, we should report the attribute classification reliability.

```{r}
ts_reli$map_reliability
```

Finally, if we report the actual probabilities for each attribute, then we should report the reliability of those probabilities.

```{r}
ts_reli$eap_reliability
```

